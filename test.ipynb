{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "from numba import cuda, jit, prange \n",
    "\n",
    "from s3ts.api.nets.encoders.dtw.dtw_no_matrix import dtw_fast_no_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(45)\n",
    "a = torch.randn(1, 2, 10)\n",
    "b = torch.randn(2, 2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def dtw_forward(dtw, w):\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "    '''\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    if x < n and y < k:\n",
    "        for i in range(1, len_pattern): # pl\n",
    "            for j in range(1, len_window): # ws\n",
    "                value = min(w * min(dtw[x, y, i, j-1], dtw[x, y, i-1, j-1]), dtw[x, y, i-1, j])\n",
    "                dtw[x, y, i, j] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def dtw_backward(dtw, dist_grad, grad):\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        dist_grad of shape (n, k, dims, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pl)\n",
    "    '''\n",
    "    n, k, d, len_pattern, len_window = dist_grad.shape\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    if x < n and y < k:\n",
    "        for i0 in range(len_pattern-1, -1, -1):\n",
    "            for j0 in range(len_window-1, -1, -1):\n",
    "\n",
    "                # A = dtw[x, y, i0, j0-1]\n",
    "                # B = dtw[x, y, i0-1, j0]\n",
    "                # C = dtw[x, y, i0-1, j0-1]\n",
    "\n",
    "                # path is A if (A<B) & (A<C) -> path is not A if (A>=B) | (A>=C)\n",
    "                # path is B if (B<A) & (B<C) -> path is not B if (B>=A) | (B>=C)\n",
    "\n",
    "                if dtw[x, y, i0, j0] != np.inf:\n",
    "\n",
    "                    for l in range(d):\n",
    "                        cuda.atomic.add(grad, (x, y, l, i0), dist_grad[x, y, l, i0, j0])      \n",
    "              \n",
    "                    if j0==0 or i0==0:\n",
    "                        continue\n",
    "\n",
    "                    if dtw[x, y, i0, j0-1] >= dtw[x, y, i0-1, j0] or dtw[x, y, i0, j0-1] >= dtw[x, y, i0-1, j0-1]: # path is not A\n",
    "                        for j in range(j0):\n",
    "                            dtw[x, y, i0, j] = np.inf\n",
    "                    if dtw[x, y, i0-1, j0] >= dtw[x, y, i0, j0-1] or dtw[x, y, i0-1, j0] >= dtw[x, y, i0-1, j0-1]: # path is not B\n",
    "                        for i in range(i0):\n",
    "                            dtw[x, y, i, j0] = np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff = a[:,None,:,None,:] - b[None,:,:,:,None]\n",
    "\n",
    "\n",
    "euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "\n",
    "# compute dtw\n",
    "euc_d[:,:,0,:] = torch.cumsum(euc_d[:,:,0,:], dim=2)\n",
    "euc_d[:,:,:,0] = torch.cumsum(euc_d[:,:,:,0], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1652, 8.8709]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euc_d[: ,:, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.zeros((1, 2, 2, 12), device=\"cuda\")\n",
    "grads_cuda = cuda.as_cuda_array(grads)\n",
    "p_diff_cuda = cuda.as_cuda_array(p_diff.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = cuda.as_cuda_array(euc_d.detach().cuda())\n",
    "dtw_forward[(16, 16), (16, 16)](dtw, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_backward[(16, 16), (16, 16)](dtw, p_diff_cuda, grads_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1809,  3.1977,  3.2883,     inf,     inf,     inf,     inf,     inf,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,  4.8867,     inf,     inf,     inf,     inf,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,     inf,  5.7061,     inf,     inf,     inf,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,     inf,  6.8401,     inf,     inf,     inf,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,     inf,  7.5416,     inf,     inf,     inf,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,     inf,     inf, 11.0243,     inf,     inf,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,     inf,     inf,     inf, 11.9491,     inf,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,     inf,     inf,     inf,     inf, 17.5958,\n",
       "             inf,     inf],\n",
       "        [    inf,     inf,     inf,     inf,     inf,     inf,     inf,     inf,\n",
       "         17.7876,     inf],\n",
       "        [    inf,     inf,     inf,     inf,     inf,     inf,     inf,     inf,\n",
       "             inf, 18.7133],\n",
       "        [    inf,     inf,     inf,     inf,     inf,     inf,     inf,     inf,\n",
       "             inf, 19.3334],\n",
       "        [    inf,     inf,     inf,     inf,     inf,     inf,     inf,     inf,\n",
       "             inf, 20.4986]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dtw.copy_to_host())[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2, 12])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0554,  1.2604,  0.4572,  1.0310, -0.8302,  0.2612,  0.1014,\n",
       "           -1.5561, -0.1494, -0.3043,  0.4818,  1.0795],\n",
       "          [-1.5693, -0.0985, -0.7813,  0.2666,  0.1108, -1.8478, -0.9563,\n",
       "           -1.7959,  0.4116,  0.9128, -0.6228, -0.0028]],\n",
       "\n",
       "         [[ 1.3708,  1.5932, -0.2534, -0.5999,  2.1670,  0.6882,  0.7017,\n",
       "            1.7478,  0.5878,  0.1707,  0.8489,  1.7637],\n",
       "          [-0.9776,  0.8694,  0.7780,  0.2931,  0.6177,  0.1789, -0.6374,\n",
       "           -0.2487,  0.7245, -1.9783, -0.1086,  2.4000]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(grads_cuda.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def dtw_compute_full(dtw: torch.Tensor, dist_grad: torch.Tensor, w: float) -> torch.Tensor:\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        dist_grad of shape (n, k, dims, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pl)\n",
    "    '''\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "    grads = torch.zeros((n, k, dist_grad.shape[2], len_pattern), device=dtw.device)\n",
    "\n",
    "    for i in range(1, len_pattern): # pl\n",
    "        for j in range(1, len_window): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[:, :, i, j-1], dtw[:, :, i-1, j-1]), dtw[:, :, i-1, j])\n",
    "\n",
    "            dtw[:, :, i, j] += value\n",
    "\n",
    "    for i0 in range(len_pattern-1, -1, -1):\n",
    "        for j0 in range(len_window-1, -1, -1):\n",
    "            mask = ~torch.isinf(dtw[:, :, i0, j0])\n",
    "            grads[:, :, :, i0][mask] += dist_grad[:, :, :, i0, j0][mask]\n",
    "\n",
    "            if j0==0 or i0==0:\n",
    "                continue\n",
    "\n",
    "            paths = torch.stack([\n",
    "                dtw[:, :, i0, j0-1],\n",
    "                dtw[:, :, i0-1, j0],\n",
    "                dtw[:, :, i0-1, j0-1]\n",
    "            ])\n",
    "\n",
    "            id = paths.argmin(0)\n",
    "\n",
    "            dtw[:, :, i0, :j0][(id!=0) & mask] = float(\"inf\")\n",
    "            dtw[:, :, :i0, j0][(id!=1) & mask] = float(\"inf\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_cpu = dtw_compute_full(euc_d, p_diff, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0554,  1.2604,  0.4572,  1.0310, -0.8302,  0.2612,  0.1014, -1.5561,\n",
       "         -0.1494, -0.3043,  0.4818,  1.0795],\n",
       "        [-1.5693, -0.0985, -0.7813,  0.2666,  0.1108, -1.8478, -0.9563, -1.7959,\n",
       "          0.4116,  0.9128, -0.6228, -0.0028]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_cpu[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_dtw_cuda(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, y: torch.Tensor, w: float):\n",
    "        DTW, p_diff = dtw_fast_cuda(x, y.detach(), w, compute_gradients=y.requires_grad)\n",
    "\n",
    "        ctx.save_for_backward(p_diff)\n",
    "\n",
    "        return DTW[:, :, -1, -1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dtw_grad):\n",
    "        # dtw_grad dims (n, k) p_diff dims (n, k, d, pl)\n",
    "        p_diff, = ctx.saved_tensors\n",
    "        mult = (p_diff * dtw_grad[:, :, None, None]) # dims (n, k, d)\n",
    "        return None, 2*mult.mean(0), None # dims (n, d, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def dtw_fill(dtw, w):\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "    '''\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    if x < n and y < k:\n",
    "        for i in range(1, len_pattern): # pl\n",
    "            for j in range(1, len_window): # ws\n",
    "                value = min(w * min(dtw[x, y, i, j-1], dtw[x, y, i-1, j-1]), dtw[x, y, i-1, j])\n",
    "                dtw[x, y, i, j] += value\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "@cuda.jit\n",
    "def dtw_backward(dtw, dist_grad, grad):\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        dist_grad of shape (n, k, dims, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pl)\n",
    "    '''\n",
    "    n, k, d, len_pattern, len_window = dist_grad.shape\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    if x < n and y < k:\n",
    "        for i0 in range(len_pattern-1, -1, -1):\n",
    "            for j0 in range(len_window-1, -1, -1):\n",
    "\n",
    "                # A = dtw[x, y, i0, j0-1]\n",
    "                # B = dtw[x, y, i0-1, j0]\n",
    "                # C = dtw[x, y, i0-1, j0-1]\n",
    "\n",
    "                # path is A if (A<B) & (A<C) -> path is not A if (A>=B) | (A>=C)\n",
    "                # path is B if (B<A) & (B<C) -> path is not B if (B>=A) | (B>=C)\n",
    "\n",
    "                if dtw[x, y, i0, j0] != np.inf:\n",
    "\n",
    "                    for l in range(d):\n",
    "                        cuda.atomic.add(grad, (x, y, l, i0), dist_grad[x, y, l, i0, j0])      \n",
    "              \n",
    "                    if j0==0 or i0==0:\n",
    "                        continue\n",
    "\n",
    "                    if dtw[x, y, i0, j0-1] >= dtw[x, y, i0-1, j0] or dtw[x, y, i0, j0-1] >= dtw[x, y, i0-1, j0-1]: # path is not A\n",
    "                        for j in range(j0):\n",
    "                            dtw[x, y, i0, j] = np.inf\n",
    "                    if dtw[x, y, i0-1, j0] >= dtw[x, y, i0, j0-1] or dtw[x, y, i0-1, j0] >= dtw[x, y, i0-1, j0-1]: # path is not B\n",
    "                        for i in range(i0):\n",
    "                            dtw[x, y, i, j0] = np.inf\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "# @torch.jit.script\n",
    "def dtw_forward(x: torch.Tensor, y: torch.Tensor, w: float):\n",
    "    # shape of x (n, dim, x_len) y (m, dim, y_len)\n",
    "\n",
    "    # performs convolution-like operation, for each kernel the DF\n",
    "    # (of shape (kernel_size, T)) is computed, then summed across channels\n",
    "    # x has shape (batch, c, time_dimension)\n",
    "\n",
    "    # compute pairwise diffs (squared)\n",
    "    p_diff = x[:,None,:,None,:] - y[None,:,:,:,None] # shape (n, n_kernel, d, Kernel_size, T)\n",
    "    euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "\n",
    "    # if compute_gradients:\n",
    "    #     p_diff /= euc_d[:,:, None, :, :] + eps\n",
    "\n",
    "    # compute dtw\n",
    "    euc_d[:,:,0,:] = torch.cumsum(euc_d[:,:,0,:], dim=2)\n",
    "    euc_d[:,:,:,0] = torch.cumsum(euc_d[:,:,:,0], dim=2)\n",
    "\n",
    "    dtw_fill[(16, 16), (16, 16)](cuda.as_cuda_array(euc_d), w)\n",
    "\n",
    "    return euc_d, p_diff\n",
    "    \n",
    "class torch_dtw_cuda(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, y: torch.Tensor, w: float = 1):\n",
    "        DTW, p_diff = dtw_forward(x, y.detach(), w)\n",
    "\n",
    "        ctx.save_for_backward(DTW, p_diff)\n",
    "\n",
    "        return DTW[:, :, -1, -1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dtw_grad):\n",
    "        # dtw_grad dims (n, k) p_diff dims (n, k, d, pl)\n",
    "        dtw, p_diff = ctx.saved_tensors\n",
    "        grads = torch.zeros((dtw.shape[0],) + p_diff.shape[1:-1], device=dtw_grad.device)\n",
    "        dtw_backward[(16, 16), (16, 16)](cuda.as_cuda_array(dtw), cuda.as_cuda_array(p_diff), cuda.as_cuda_array(grads))\n",
    "\n",
    "        mult = (dtw_grad[:, :, None, None] * grads) # dims (n, k, d)\n",
    "        return None, 2*mult.mean(0), None # dims (n, d, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4, 15).cuda()\n",
    "y = torch.randn(5, 4, 20, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch_dtw_cuda.apply(x, y.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "res2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 20])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y._grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssstsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
