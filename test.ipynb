{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "from numba import cuda, jit, prange \n",
    "\n",
    "from s3ts.api.nets.encoders.dtw.dtw_no_matrix import dtw_fast_no_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(45)\n",
    "a = torch.randn(16, 5, 32)\n",
    "b = torch.randn(5, 5, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def dtw_forward(dtw, w):\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "    '''\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    if x < n and y < k:\n",
    "        for i in range(1, len_pattern): # pl\n",
    "            for j in range(1, len_window): # ws\n",
    "                value = min(w * min(dtw[x, y, i, j-1], dtw[x, y, i-1, j-1]), dtw[x, y, i-1, j])\n",
    "                dtw[x, y, i, j] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def dtw_backward(dtw, dist_grad, grad):\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        dist_grad of shape (n, k, dims, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pl)\n",
    "    '''\n",
    "    n, k, d, len_pattern, len_window = dist_grad.shape\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    if x < n and y < k:\n",
    "        for i0 in range(len_pattern-1, -1, -1):\n",
    "            for j0 in range(len_window-1, -1, -1):\n",
    "\n",
    "                # A = dtw[x, y, i0, j0-1]\n",
    "                # B = dtw[x, y, i0-1, j0]\n",
    "                # C = dtw[x, y, i0-1, j0-1]\n",
    "\n",
    "                # path is A if (A<B) & (A<C) -> path is not A if (A>=B) | (A>=C)\n",
    "                # path is B if (B<A) & (B<C) -> path is not B if (B>=A) | (B>=C)\n",
    "\n",
    "                if dtw[x, y, i0, j0] != np.inf:\n",
    "\n",
    "                    for l in range(d):\n",
    "                        cuda.atomic.add(grad, (x, y, l, i0), dist_grad[x, y, l, i0, j0])      \n",
    "              \n",
    "                    if j0==0 or i0==0:\n",
    "                        continue\n",
    "\n",
    "                    if dtw[x, y, i0, j0-1] >= dtw[x, y, i0-1, j0] or dtw[x, y, i0, j0-1] >= dtw[x, y, i0-1, j0-1]: # path is not A\n",
    "                        for j in range(j0):\n",
    "                            dtw[x, y, i0, j] = np.inf\n",
    "                    if dtw[x, y, i0-1, j0] >= dtw[x, y, i0, j0-1] or dtw[x, y, i0-1, j0] >= dtw[x, y, i0-1, j0-1]: # path is not B\n",
    "                        for i in range(i0):\n",
    "                            dtw[x, y, i, j0] = np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diff = a[:,None,:,None,:] - b[None,:,:,:,None]\n",
    "\n",
    "\n",
    "euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "p_diff /= torch.sqrt(euc_d[:,:, None, :, :] + 1e-6)\n",
    "\n",
    "# compute dtw\n",
    "euc_d[:,:,0,:] = torch.cumsum(euc_d[:,:,0,:], dim=2)\n",
    "euc_d[:,:,:,0] = torch.cumsum(euc_d[:,:,:,0], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.3049, 12.5326, 14.2390, 18.5499, 21.3878],\n",
       "        [11.3199, 14.6318,  3.1673,  6.2476,  7.9558],\n",
       "        [ 2.9047, 13.5166,  2.5900,  7.3420, 16.9095],\n",
       "        [11.8028, 19.0041,  5.9039,  7.2419, 11.8238],\n",
       "        [ 5.6307,  5.2362,  6.0658, 14.2877,  8.3812],\n",
       "        [11.2682,  8.7904,  6.7414, 14.0469,  9.2293],\n",
       "        [15.1081,  8.2353, 16.9080, 25.9967, 10.8065],\n",
       "        [32.1689, 31.2574, 15.9637, 16.0273, 33.0433],\n",
       "        [18.0312, 19.2785,  6.0626,  5.7663, 14.8308],\n",
       "        [26.5202, 24.1122, 14.4008, 13.6005, 12.0924],\n",
       "        [18.6912, 12.9780, 12.8129, 17.3771, 17.1096],\n",
       "        [11.3255, 24.8915,  2.4223,  3.5488, 27.4078],\n",
       "        [ 8.2778, 18.2923, 10.6480, 13.8823, 18.5577],\n",
       "        [ 6.9313,  9.3066,  2.8754,  7.0702, 14.7061],\n",
       "        [ 5.4527, 14.2640,  4.5757,  8.3152, 14.1447],\n",
       "        [ 8.1081, 15.0456,  0.7441,  2.7304, 17.6448]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euc_d[: ,:, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.zeros((16, 5, 5, 64), device=\"cuda\")\n",
    "grads_cuda = cuda.as_cuda_array(grads)\n",
    "p_diff_cuda = cuda.as_cuda_array(p_diff.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = cuda.as_cuda_array(euc_d.detach().cuda())\n",
    "dtw_forward[(16, 16), (16, 16)](dtw, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_backward[(16, 16), (16, 16)](dtw, p_diff_cuda, grads_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9.3449,      inf,      inf,  ...,      inf,      inf,      inf],\n",
       "        [     inf,  15.8723,      inf,  ...,      inf,      inf,      inf],\n",
       "        [     inf,      inf,  19.5062,  ...,      inf,      inf,      inf],\n",
       "        ...,\n",
       "        [     inf,      inf,      inf,  ...,      inf,      inf, 387.8178],\n",
       "        [     inf,      inf,      inf,  ...,      inf,      inf, 408.5746],\n",
       "        [     inf,      inf,      inf,  ...,      inf,      inf, 416.8795]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dtw.copy_to_host())[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5245e-03,  4.6715e-01,  9.3521e-01, -4.2035e-01, -3.8310e-01,\n",
       "         -8.0012e-01, -2.1795e-01,  7.1087e-01,  4.9632e-01,  5.0801e-01,\n",
       "          2.3749e-01,  7.4432e-01,  5.6123e-01,  5.5381e-01,  8.0801e-01,\n",
       "         -4.5157e-01,  2.4809e-01,  7.4112e-01, -2.8445e-01, -1.7460e-02,\n",
       "         -2.1852e-01,  6.7370e-03, -5.7965e-01,  6.2305e-01, -8.9107e-02,\n",
       "          6.0938e-01,  1.0445e-01, -6.8408e-01, -2.8676e-01, -2.7234e-01,\n",
       "          1.4446e-01, -2.1205e-01, -4.7354e-01, -3.4677e-01, -7.4158e-02,\n",
       "         -1.8458e-01,  7.6073e-02, -2.7699e-01, -4.7787e-01,  4.2296e-04,\n",
       "         -9.4835e-01, -4.7182e-01, -5.7164e-01, -3.2118e-01,  7.5695e-03,\n",
       "         -2.0316e-01,  8.8038e-02, -2.7533e-01,  3.3248e-01,  5.7210e-01,\n",
       "          6.6940e-01, -1.1434e-01,  1.1283e-01,  2.9612e-01,  4.3339e-01,\n",
       "         -3.0245e-01, -5.4845e-01,  3.5049e-01, -2.8474e-01,  2.1408e-01,\n",
       "          8.2052e-01, -4.5766e-01, -7.0214e-01, -6.1360e-01],\n",
       "        [-7.2771e-01,  8.0601e-01,  7.6242e-02,  3.5868e-01,  1.1087e-01,\n",
       "         -1.0748e-01,  8.9276e-01,  4.5862e-01,  4.3084e-01,  4.4217e-01,\n",
       "          3.6071e-01, -3.1502e-01,  1.3074e-01, -5.1270e-02,  5.2916e-01,\n",
       "          8.7026e-01,  5.1281e-01, -4.7283e-01, -4.0340e-02, -4.6366e-01,\n",
       "         -7.4002e-01, -5.8184e-01, -3.2870e-01, -3.7726e-01,  8.0961e-01,\n",
       "         -6.9643e-01,  6.3792e-01,  2.6770e-01,  2.3691e-01, -1.9385e-01,\n",
       "          2.0614e-01,  7.5528e-01, -5.2355e-01,  1.4266e-01, -2.5193e-02,\n",
       "          2.9331e-01,  5.3251e-01,  2.7374e-02, -1.7898e-01,  7.1818e-01,\n",
       "          2.6642e-01, -1.8167e-01, -2.9059e-01,  9.0139e-01, -2.5799e-01,\n",
       "         -7.8067e-01, -8.2198e-01, -8.8965e-01, -7.7898e-01,  2.1965e-01,\n",
       "          4.6552e-01,  1.8258e-01, -5.7863e-01, -7.5732e-01, -5.4867e-01,\n",
       "         -6.4499e-01, -9.7500e-02, -2.7490e-01,  5.8186e-02, -6.1979e-01,\n",
       "         -3.2649e-01,  8.7575e-01,  3.5618e-01,  1.3536e-01],\n",
       "        [-1.1181e-01,  1.5099e-01,  3.0725e-01, -8.5761e-01, -3.1061e-01,\n",
       "         -5.2069e-01,  3.8602e-01, -2.8876e-01, -6.8040e-02, -3.2340e-01,\n",
       "         -3.1155e-01,  3.1130e-01, -2.1327e-01,  5.8868e-01, -2.0877e-01,\n",
       "         -1.1518e-01,  1.3629e-01,  1.8904e-01,  2.8980e-01,  2.4967e-01,\n",
       "         -4.3012e-01, -1.3173e-01,  1.0418e-01,  3.1469e-01,  1.5792e-02,\n",
       "          2.4909e-01,  6.2637e-01,  5.8327e-01,  5.8993e-01,  3.1213e-01,\n",
       "          7.9964e-01,  1.4713e-01,  2.2405e-01,  8.9586e-01,  8.4665e-01,\n",
       "         -6.3963e-01,  6.0135e-01,  9.8740e-02,  8.4053e-02,  3.4946e-01,\n",
       "          6.8133e-02, -7.2890e-01,  5.1582e-01,  1.9152e-01,  7.8337e-01,\n",
       "          3.6929e-01,  2.8868e-01,  2.2766e-01, -5.1907e-01,  6.4643e-01,\n",
       "         -5.5209e-03,  7.3405e-01, -3.8423e-01, -1.5379e-01,  4.9119e-02,\n",
       "          6.6065e-01, -1.7835e-01,  4.7500e-01,  8.2863e-01,  7.1182e-01,\n",
       "          4.5644e-01, -1.4985e-01,  5.5383e-01, -2.8725e-01],\n",
       "        [-6.7499e-01, -7.4962e-02,  1.1266e-01, -4.0908e-01,  3.4150e-01,\n",
       "          2.7361e-01, -2.2352e-02, -1.0196e-01, -4.2643e-01,  6.6283e-01,\n",
       "          6.2433e-01,  4.3132e-01,  1.7497e-01,  5.8144e-01,  1.2439e-01,\n",
       "          1.2227e-01,  6.3320e-01, -7.5795e-02, -6.2735e-01, -3.8268e-01,\n",
       "          1.1979e-02, -1.0344e-01, -1.1667e+00,  3.8199e-01,  8.4942e-02,\n",
       "          2.8351e-01,  2.8300e-01, -7.9658e-02,  1.6757e-01,  3.5432e-01,\n",
       "         -5.1175e-01, -5.6167e-01, -6.6966e-01,  2.2372e-01,  2.8617e-01,\n",
       "         -6.1824e-01,  5.7283e-01, -3.3888e-01,  7.5906e-01,  5.6464e-01,\n",
       "          1.5814e-01, -4.3287e-01, -5.6777e-01,  2.1815e-01,  3.4693e-02,\n",
       "         -4.8812e-02,  4.4887e-01, -2.0190e-01, -8.1062e-02, -3.7734e-01,\n",
       "          1.4329e-01, -6.2800e-01,  4.4701e-01, -3.5023e-01,  3.7772e-01,\n",
       "         -2.3144e-01,  6.8967e-01,  6.7744e-01,  4.7631e-01,  1.1833e-01,\n",
       "         -9.3729e-02, -2.9761e-02, -1.1411e-01, -6.0244e-01],\n",
       "        [ 4.7517e-02, -3.2204e-01, -1.1170e-01, -2.3261e+00, -7.9237e-01,\n",
       "         -4.7740e-02,  7.7223e-02, -4.3653e-01,  6.1772e-01,  4.9856e-02,\n",
       "          5.7152e-01, -2.5257e-01, -7.6931e-01,  7.7835e-02, -8.9748e-02,\n",
       "          1.0255e-01, -5.0592e-01, -4.3092e-01, -6.6325e-01, -7.5890e-01,\n",
       "          4.6848e-01, -7.9584e-01,  9.0624e-01, -4.7385e-01,  5.7369e-01,\n",
       "          3.4888e-02,  3.3123e-01,  3.3737e-01, -6.9681e-01,  8.1565e-01,\n",
       "         -1.8797e-01, -2.1785e-01,  5.4894e-02, -8.2368e-02, -4.4177e-01,\n",
       "          2.9758e-01, -1.4455e-01,  8.9327e-01, -3.9543e-01,  2.0804e-01,\n",
       "         -9.2413e-04, -1.6036e-01, -1.8963e-02, -8.4793e-03, -5.6437e-01,\n",
       "          4.5883e-01, -1.7824e-01,  2.0032e-01,  8.1510e-02,  2.5339e-01,\n",
       "         -5.6092e-01, -1.4278e-01, -5.5227e-01,  4.3871e-01,  6.0502e-01,\n",
       "          4.9896e-02, -4.2690e-01,  3.4211e-01, -4.5146e-02,  2.2212e-01,\n",
       "         -5.5026e-02, -1.6630e-02,  2.4572e-01,  3.9966e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5245e-03,  4.6715e-01,  9.3521e-01, -4.2035e-01, -3.8310e-01,\n",
       "         -8.0012e-01, -2.1795e-01,  7.1087e-01,  4.9632e-01,  5.0801e-01,\n",
       "          2.3749e-01,  7.4432e-01,  5.6123e-01,  5.5381e-01,  8.0801e-01,\n",
       "         -4.5157e-01,  2.4809e-01,  7.4112e-01, -2.8445e-01, -1.7460e-02,\n",
       "         -2.1852e-01,  6.7370e-03, -5.7965e-01,  6.2305e-01, -8.9107e-02,\n",
       "          6.0938e-01,  1.0445e-01, -6.8408e-01, -2.8676e-01, -2.7234e-01,\n",
       "          1.4446e-01, -2.1205e-01, -4.7354e-01, -3.4677e-01, -7.4158e-02,\n",
       "         -1.8458e-01,  7.6073e-02, -2.7699e-01, -4.7787e-01,  4.2296e-04,\n",
       "         -9.4835e-01, -4.7182e-01, -5.7164e-01, -3.2118e-01,  7.5695e-03,\n",
       "         -2.0316e-01,  8.8038e-02, -2.7533e-01,  3.3248e-01,  5.7210e-01,\n",
       "          6.6940e-01, -1.1434e-01,  1.1283e-01,  2.9612e-01,  4.3339e-01,\n",
       "         -3.0245e-01, -5.4845e-01,  3.5049e-01, -2.8474e-01,  2.1408e-01,\n",
       "          8.2052e-01, -4.5766e-01, -7.0214e-01, -6.1360e-01],\n",
       "        [-7.2771e-01,  8.0601e-01,  7.6242e-02,  3.5868e-01,  1.1087e-01,\n",
       "         -1.0748e-01,  8.9276e-01,  4.5862e-01,  4.3084e-01,  4.4217e-01,\n",
       "          3.6071e-01, -3.1502e-01,  1.3074e-01, -5.1270e-02,  5.2916e-01,\n",
       "          8.7026e-01,  5.1281e-01, -4.7283e-01, -4.0340e-02, -4.6366e-01,\n",
       "         -7.4002e-01, -5.8184e-01, -3.2870e-01, -3.7726e-01,  8.0961e-01,\n",
       "         -6.9643e-01,  6.3792e-01,  2.6770e-01,  2.3691e-01, -1.9385e-01,\n",
       "          2.0614e-01,  7.5528e-01, -5.2355e-01,  1.4266e-01, -2.5193e-02,\n",
       "          2.9331e-01,  5.3251e-01,  2.7374e-02, -1.7898e-01,  7.1818e-01,\n",
       "          2.6642e-01, -1.8167e-01, -2.9059e-01,  9.0139e-01, -2.5799e-01,\n",
       "         -7.8067e-01, -8.2198e-01, -8.8965e-01, -7.7898e-01,  2.1965e-01,\n",
       "          4.6552e-01,  1.8258e-01, -5.7863e-01, -7.5732e-01, -5.4867e-01,\n",
       "         -6.4499e-01, -9.7500e-02, -2.7490e-01,  5.8186e-02, -6.1979e-01,\n",
       "         -3.2649e-01,  8.7575e-01,  3.5618e-01,  1.3536e-01],\n",
       "        [-1.1181e-01,  1.5099e-01,  3.0725e-01, -8.5761e-01, -3.1061e-01,\n",
       "         -5.2069e-01,  3.8602e-01, -2.8876e-01, -6.8040e-02, -3.2340e-01,\n",
       "         -3.1155e-01,  3.1130e-01, -2.1327e-01,  5.8868e-01, -2.0877e-01,\n",
       "         -1.1518e-01,  1.3629e-01,  1.8904e-01,  2.8980e-01,  2.4967e-01,\n",
       "         -4.3012e-01, -1.3173e-01,  1.0418e-01,  3.1469e-01,  1.5792e-02,\n",
       "          2.4909e-01,  6.2637e-01,  5.8327e-01,  5.8993e-01,  3.1213e-01,\n",
       "          7.9964e-01,  1.4713e-01,  2.2405e-01,  8.9586e-01,  8.4665e-01,\n",
       "         -6.3963e-01,  6.0135e-01,  9.8740e-02,  8.4053e-02,  3.4946e-01,\n",
       "          6.8133e-02, -7.2890e-01,  5.1582e-01,  1.9152e-01,  7.8337e-01,\n",
       "          3.6929e-01,  2.8868e-01,  2.2766e-01, -5.1907e-01,  6.4643e-01,\n",
       "         -5.5209e-03,  7.3405e-01, -3.8423e-01, -1.5379e-01,  4.9119e-02,\n",
       "          6.6065e-01, -1.7835e-01,  4.7500e-01,  8.2863e-01,  7.1182e-01,\n",
       "          4.5644e-01, -1.4985e-01,  5.5383e-01, -2.8725e-01],\n",
       "        [-6.7499e-01, -7.4962e-02,  1.1266e-01, -4.0908e-01,  3.4150e-01,\n",
       "          2.7361e-01, -2.2352e-02, -1.0196e-01, -4.2643e-01,  6.6283e-01,\n",
       "          6.2433e-01,  4.3132e-01,  1.7497e-01,  5.8144e-01,  1.2439e-01,\n",
       "          1.2227e-01,  6.3320e-01, -7.5795e-02, -6.2735e-01, -3.8268e-01,\n",
       "          1.1979e-02, -1.0344e-01, -1.1667e+00,  3.8199e-01,  8.4942e-02,\n",
       "          2.8351e-01,  2.8300e-01, -7.9658e-02,  1.6757e-01,  3.5432e-01,\n",
       "         -5.1175e-01, -5.6167e-01, -6.6966e-01,  2.2372e-01,  2.8617e-01,\n",
       "         -6.1824e-01,  5.7283e-01, -3.3888e-01,  7.5906e-01,  5.6464e-01,\n",
       "          1.5814e-01, -4.3287e-01, -5.6777e-01,  2.1815e-01,  3.4693e-02,\n",
       "         -4.8812e-02,  4.4887e-01, -2.0190e-01, -8.1062e-02, -3.7734e-01,\n",
       "          1.4329e-01, -6.2800e-01,  4.4701e-01, -3.5023e-01,  3.7772e-01,\n",
       "         -2.3144e-01,  6.8967e-01,  6.7744e-01,  4.7631e-01,  1.1833e-01,\n",
       "         -9.3729e-02, -2.9761e-02, -1.1411e-01, -6.0244e-01],\n",
       "        [ 4.7517e-02, -3.2204e-01, -1.1170e-01, -2.3261e+00, -7.9237e-01,\n",
       "         -4.7740e-02,  7.7223e-02, -4.3653e-01,  6.1772e-01,  4.9856e-02,\n",
       "          5.7152e-01, -2.5257e-01, -7.6931e-01,  7.7835e-02, -8.9748e-02,\n",
       "          1.0255e-01, -5.0592e-01, -4.3092e-01, -6.6325e-01, -7.5890e-01,\n",
       "          4.6848e-01, -7.9584e-01,  9.0624e-01, -4.7385e-01,  5.7369e-01,\n",
       "          3.4888e-02,  3.3123e-01,  3.3737e-01, -6.9681e-01,  8.1565e-01,\n",
       "         -1.8797e-01, -2.1785e-01,  5.4894e-02, -8.2368e-02, -4.4177e-01,\n",
       "          2.9758e-01, -1.4455e-01,  8.9327e-01, -3.9543e-01,  2.0804e-01,\n",
       "         -9.2413e-04, -1.6036e-01, -1.8963e-02, -8.4793e-03, -5.6437e-01,\n",
       "          4.5883e-01, -1.7824e-01,  2.0032e-01,  8.1510e-02,  2.5339e-01,\n",
       "         -5.6092e-01, -1.4278e-01, -5.5227e-01,  4.3871e-01,  6.0502e-01,\n",
       "          4.9896e-02, -4.2690e-01,  3.4211e-01, -4.5146e-02,  2.2212e-01,\n",
       "         -5.5026e-02, -1.6630e-02,  2.4572e-01,  3.9966e-01]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(grads_cuda.copy_to_host())[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def dtw_compute_full(dtw: torch.Tensor, dist_grad: torch.Tensor, w: float) -> torch.Tensor:\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        dist_grad of shape (n, k, dims, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pl)\n",
    "    '''\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "    grads = torch.zeros((n, k, dist_grad.shape[2], len_pattern), device=dtw.device)\n",
    "\n",
    "    for i in range(1, len_pattern): # pl\n",
    "        for j in range(1, len_window): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[:, :, i, j-1], dtw[:, :, i-1, j-1]), dtw[:, :, i-1, j])\n",
    "\n",
    "            dtw[:, :, i, j] += value\n",
    "\n",
    "    for i0 in range(len_pattern-1, -1, -1):\n",
    "        for j0 in range(len_window-1, -1, -1):\n",
    "            mask = ~torch.isinf(dtw[:, :, i0, j0])\n",
    "            grads[:, :, :, i0][mask] += dist_grad[:, :, :, i0, j0][mask]\n",
    "\n",
    "            if j0==0 or i0==0:\n",
    "                continue\n",
    "\n",
    "            paths = torch.stack([\n",
    "                dtw[:, :, i0, j0-1],\n",
    "                dtw[:, :, i0-1, j0],\n",
    "                dtw[:, :, i0-1, j0-1]\n",
    "            ])\n",
    "\n",
    "            id = paths.argmin(0)\n",
    "\n",
    "            dtw[:, :, i0, :j0][(id!=0) & mask] = float(\"inf\")\n",
    "            dtw[:, :, :i0, j0][(id!=1) & mask] = float(\"inf\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_cpu = dtw_compute_full(euc_d, p_diff, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5245e-03,  4.6715e-01,  9.3521e-01, -4.2035e-01, -3.8310e-01,\n",
       "         -8.0012e-01, -2.1795e-01,  7.1087e-01,  4.9632e-01,  5.0801e-01,\n",
       "          2.3749e-01,  7.4432e-01,  5.6123e-01,  5.5381e-01,  8.0801e-01,\n",
       "         -4.5157e-01,  2.4809e-01,  7.4112e-01, -2.8445e-01, -1.7460e-02,\n",
       "         -2.1852e-01,  6.7370e-03, -5.7965e-01,  6.2305e-01, -8.9107e-02,\n",
       "          6.0938e-01,  1.0445e-01, -6.8408e-01, -2.8676e-01, -2.7234e-01,\n",
       "          1.4446e-01, -2.1205e-01, -4.7354e-01, -3.4677e-01, -7.4158e-02,\n",
       "         -1.8458e-01,  7.6073e-02, -2.7699e-01, -4.7787e-01,  4.2296e-04,\n",
       "         -9.4835e-01, -4.7182e-01, -5.7164e-01, -3.2118e-01,  7.5695e-03,\n",
       "         -2.0316e-01,  8.8038e-02, -2.7533e-01,  3.3248e-01,  5.7210e-01,\n",
       "          6.6940e-01, -1.1434e-01,  1.1283e-01,  2.9612e-01,  4.3339e-01,\n",
       "         -3.0245e-01, -5.4845e-01,  3.5049e-01, -2.8474e-01,  2.1408e-01,\n",
       "          8.2052e-01, -4.5766e-01, -7.0214e-01, -6.1360e-01],\n",
       "        [-7.2771e-01,  8.0601e-01,  7.6242e-02,  3.5868e-01,  1.1087e-01,\n",
       "         -1.0748e-01,  8.9276e-01,  4.5862e-01,  4.3084e-01,  4.4217e-01,\n",
       "          3.6071e-01, -3.1502e-01,  1.3074e-01, -5.1270e-02,  5.2916e-01,\n",
       "          8.7026e-01,  5.1281e-01, -4.7283e-01, -4.0340e-02, -4.6366e-01,\n",
       "         -7.4002e-01, -5.8184e-01, -3.2870e-01, -3.7726e-01,  8.0961e-01,\n",
       "         -6.9643e-01,  6.3792e-01,  2.6770e-01,  2.3691e-01, -1.9385e-01,\n",
       "          2.0614e-01,  7.5528e-01, -5.2355e-01,  1.4266e-01, -2.5193e-02,\n",
       "          2.9331e-01,  5.3251e-01,  2.7374e-02, -1.7898e-01,  7.1818e-01,\n",
       "          2.6642e-01, -1.8167e-01, -2.9059e-01,  9.0139e-01, -2.5799e-01,\n",
       "         -7.8067e-01, -8.2198e-01, -8.8965e-01, -7.7898e-01,  2.1965e-01,\n",
       "          4.6552e-01,  1.8258e-01, -5.7863e-01, -7.5732e-01, -5.4867e-01,\n",
       "         -6.4499e-01, -9.7500e-02, -2.7490e-01,  5.8186e-02, -6.1979e-01,\n",
       "         -3.2649e-01,  8.7575e-01,  3.5618e-01,  1.3536e-01],\n",
       "        [-1.1181e-01,  1.5099e-01,  3.0725e-01, -8.5761e-01, -3.1061e-01,\n",
       "         -5.2069e-01,  3.8602e-01, -2.8876e-01, -6.8040e-02, -3.2340e-01,\n",
       "         -3.1155e-01,  3.1130e-01, -2.1327e-01,  5.8868e-01, -2.0877e-01,\n",
       "         -1.1518e-01,  1.3629e-01,  1.8904e-01,  2.8980e-01,  2.4967e-01,\n",
       "         -4.3012e-01, -1.3173e-01,  1.0418e-01,  3.1469e-01,  1.5792e-02,\n",
       "          2.4909e-01,  6.2637e-01,  5.8327e-01,  5.8993e-01,  3.1213e-01,\n",
       "          7.9964e-01,  1.4713e-01,  2.2405e-01,  8.9586e-01,  8.4665e-01,\n",
       "         -6.3963e-01,  6.0135e-01,  9.8740e-02,  8.4053e-02,  3.4946e-01,\n",
       "          6.8133e-02, -7.2890e-01,  5.1582e-01,  1.9152e-01,  7.8337e-01,\n",
       "          3.6929e-01,  2.8868e-01,  2.2766e-01, -5.1907e-01,  6.4643e-01,\n",
       "         -5.5209e-03,  7.3405e-01, -3.8423e-01, -1.5379e-01,  4.9119e-02,\n",
       "          6.6065e-01, -1.7835e-01,  4.7500e-01,  8.2863e-01,  7.1182e-01,\n",
       "          4.5644e-01, -1.4985e-01,  5.5383e-01, -2.8725e-01],\n",
       "        [-6.7499e-01, -7.4962e-02,  1.1266e-01, -4.0908e-01,  3.4150e-01,\n",
       "          2.7361e-01, -2.2352e-02, -1.0196e-01, -4.2643e-01,  6.6283e-01,\n",
       "          6.2433e-01,  4.3132e-01,  1.7497e-01,  5.8144e-01,  1.2439e-01,\n",
       "          1.2227e-01,  6.3320e-01, -7.5795e-02, -6.2735e-01, -3.8268e-01,\n",
       "          1.1979e-02, -1.0344e-01, -1.1667e+00,  3.8199e-01,  8.4942e-02,\n",
       "          2.8351e-01,  2.8300e-01, -7.9658e-02,  1.6757e-01,  3.5432e-01,\n",
       "         -5.1175e-01, -5.6167e-01, -6.6966e-01,  2.2372e-01,  2.8617e-01,\n",
       "         -6.1824e-01,  5.7283e-01, -3.3888e-01,  7.5906e-01,  5.6464e-01,\n",
       "          1.5814e-01, -4.3287e-01, -5.6777e-01,  2.1815e-01,  3.4693e-02,\n",
       "         -4.8812e-02,  4.4887e-01, -2.0190e-01, -8.1062e-02, -3.7734e-01,\n",
       "          1.4329e-01, -6.2800e-01,  4.4701e-01, -3.5023e-01,  3.7772e-01,\n",
       "         -2.3144e-01,  6.8967e-01,  6.7744e-01,  4.7631e-01,  1.1833e-01,\n",
       "         -9.3729e-02, -2.9761e-02, -1.1411e-01, -6.0244e-01],\n",
       "        [ 4.7517e-02, -3.2204e-01, -1.1170e-01, -2.3261e+00, -7.9237e-01,\n",
       "         -4.7740e-02,  7.7223e-02, -4.3653e-01,  6.1772e-01,  4.9856e-02,\n",
       "          5.7152e-01, -2.5257e-01, -7.6931e-01,  7.7835e-02, -8.9748e-02,\n",
       "          1.0255e-01, -5.0592e-01, -4.3092e-01, -6.6325e-01, -7.5890e-01,\n",
       "          4.6848e-01, -7.9584e-01,  9.0624e-01, -4.7385e-01,  5.7369e-01,\n",
       "          3.4888e-02,  3.3123e-01,  3.3737e-01, -6.9681e-01,  8.1565e-01,\n",
       "         -1.8797e-01, -2.1785e-01,  5.4894e-02, -8.2368e-02, -4.4177e-01,\n",
       "          2.9758e-01, -1.4455e-01,  8.9327e-01, -3.9543e-01,  2.0804e-01,\n",
       "         -9.2413e-04, -1.6036e-01, -1.8963e-02, -8.4793e-03, -5.6437e-01,\n",
       "          4.5883e-01, -1.7824e-01,  2.0032e-01,  8.1510e-02,  2.5339e-01,\n",
       "         -5.6092e-01, -1.4278e-01, -5.5227e-01,  4.3871e-01,  6.0502e-01,\n",
       "          4.9896e-02, -4.2690e-01,  3.4211e-01, -4.5146e-02,  2.2212e-01,\n",
       "         -5.5026e-02, -1.6630e-02,  2.4572e-01,  3.9966e-01]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_cpu[0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssstsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
