{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from storage.har_datasets import HARTHDataset, UCI_HARDataset, sts_medoids, StreamingTimeSeriesCopy, split_by_test_subject\n",
    "from s3ts.api.dms.har_datasets import LDFDataset, DFDataset\n",
    "from storage.label_mappings import harth_label_mapping\n",
    "from s3ts.api.nets.methods import create_model_from_DM, train_model, test_model\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6225494\n"
     ]
    }
   ],
   "source": [
    "ds = HARTHDataset(\"./datasets/HARTH/\", wsize=48, normalize=True, label_mapping=harth_label_mapping)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dissimilarity frames if available...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./datasets/HARTH/meds.npz\"):\n",
    "    meds = sts_medoids(ds, n=500)\n",
    "    with open(\"./datasets/HARTH/meds.npz\", \"wb\") as f:\n",
    "        np.save(f, meds)\n",
    "else:\n",
    "    meds = np.load(\"./datasets/HARTH/meds.npz\")\n",
    "\n",
    "dfds = DFDataset(ds, patterns=meds, w=0.1, dm_transform=None, ram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in np.random.choice(np.arange(len(dfds)), 500):\n",
    "    dm, _, _ = dfds[i]\n",
    "    DM.append(dm)\n",
    "\n",
    "DM = torch.stack(DM)\n",
    "\n",
    "dm_transform = Normalize(mean=DM.mean(dim=[0, 2, 3]), std=DM.std(dim=[0, 2, 3]))\n",
    "dfds.dm_transform = dm_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_split \u001b[38;5;241m=\u001b[39m split_by_test_subject(ds, \u001b[38;5;241m21\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dm \u001b[38;5;241m=\u001b[39m \u001b[43mLDFDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_inbalance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/s3ts/s3ts/api/dms/har_datasets.py:193\u001b[0m, in \u001b[0;36mLDFDataset.__init__\u001b[0;34m(self, dfds, data_split, batch_size, random_seed, num_workers, reduce_inbalance)\u001b[0m\n\u001b[1;32m    190\u001b[0m val_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(total_observations)[data_split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfds\u001b[38;5;241m.\u001b[39mstsds\u001b[38;5;241m.\u001b[39mindices)]\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_inbalance:\n\u001b[0;32m--> 193\u001b[0m     train_indices \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_inbalance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstsds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstsds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_train \u001b[38;5;241m=\u001b[39m DFDatasetCopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfds, train_indices)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_test \u001b[38;5;241m=\u001b[39m DFDatasetCopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfds, test_indices)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "data_split = split_by_test_subject(ds, 21)\n",
    "\n",
    "dm = LDFDataset(dfds, data_split=data_split, batch_size=128, random_seed=42, num_workers=8, reduce_train_inbalance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6047162\n",
      "178332\n",
      "178332\n"
     ]
    }
   ],
   "source": [
    "print(len(dm.ds_train))\n",
    "print(len(dm.ds_val))\n",
    "print(len(dm.ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 12, 48, 48])\n",
      "Latent shape:  torch.Size([1, 40, 3, 48])\n"
     ]
    }
   ],
   "source": [
    "model = create_model_from_DM(dm, name=None, \n",
    "        dsrc=\"img\", arch=\"cnn\", task=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = train_model(dm, model, max_epochs=2)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssstsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
