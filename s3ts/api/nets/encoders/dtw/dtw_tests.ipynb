{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.multiprocessing import Pool, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.3206) tensor(7.9872)\n"
     ]
    }
   ],
   "source": [
    "from dtw import dtw_fast_no_n as dtw_original\n",
    "\n",
    "torch.random.manual_seed(45)\n",
    "x = torch.randn(2, 10, 5)\n",
    "y = torch.randn(2, 10, 4)\n",
    "\n",
    "a, b = dtw_original(x, y, w=0.01, eps=1e-6)\n",
    "print(b.min(), b.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import dtw_fast_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.1949) tensor(2.3286)\n"
     ]
    }
   ],
   "source": [
    "a, b = dtw_fast_full(x, y, w=0.01, eps=1e-6)\n",
    "print(b.min(), b.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.jit.script\n",
    "def dtw_compute_all_script(dtw: torch.Tensor, dist_grad: torch.Tensor, grad: torch.Tensor, w: float) -> None:\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        dist_grad of shape (n, k, dims, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pattern_len)\n",
    "        grads of shape (n, k, dims, pattern_len, pattern_len, window_size)\n",
    "    '''\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "    # very big tensor\n",
    "    # grads shape (pattern_len(2), window_size, n, k, dims, pattern_len)\n",
    "    grads = torch.zeros(len_pattern, len_window, n, k, grad.shape[2], len_pattern, device=grad.device)\n",
    "    temp = torch.cumsum(dist_grad, dim=4).permute(3, 4, 0, 1, 2) # (pattern_len, window_size, n, k, dim)\n",
    "\n",
    "    for i in range(len_pattern):\n",
    "        grads[i, :, :, :, :, i] = temp[i, :, :, :, :]\n",
    "        grads[i:, 0, :, :, :, i] = temp[i, :1, :, :, :]\n",
    "\n",
    "    for i in range(1, len_pattern): # pl\n",
    "        for j in range(1, len_window): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[:, :, i, j-1], dtw[:, :, i-1, j-1]), dtw[:, :, i-1, j])\n",
    "            temp_1 = dtw[:, :, i, j-1] < dtw[:, :, i-1, j-1] # path (i, j-1) or (i-1, j)\n",
    "            temp_2 = w * dtw[:, :, i, j-1] < dtw[:, :, i-1, j] # path (i, j-1) or (i-1, j-1)\n",
    "            temp_3 = w * dtw[:, :, i-1, j-1] < dtw[:, :, i-1, j] # path (i-1, j-1) or (i-1, j)\n",
    "\n",
    "            dtw[:, :, i, j] += value\n",
    "\n",
    "            print(temp_1.shape, grads[i, j].shape)\n",
    "            grads[i, j][temp_1 & temp_2] += w * grads[i, j-1][temp_1 & temp_2]\n",
    "            grads[i, j][temp_1 & temp_3] += grads[i-1, j][temp_1 & temp_3]\n",
    "            grads[i, j][temp_2 & temp_3] += w * grads[i-1, j-1][temp_2 & temp_3]\n",
    "\n",
    "    grad += grads.sum(dim=(0, 1))\n",
    "\n",
    "def dtw_compute_all(dtw: torch.Tensor, dist_grad: torch.Tensor, grad: torch.Tensor, w: float) -> None:\n",
    "    dtw_compute_all_script(dtw, dist_grad, grad, w)\n",
    "\n",
    "@torch.jit.script\n",
    "def dtw_compute_no_grad(dtw: torch.Tensor, w: float) -> None:\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pattern_len)\n",
    "    '''\n",
    "\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "\n",
    "    for i in range(1, len_pattern): # pl\n",
    "        for j in range(1, len_window): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[:, :, i, j-1], dtw[:, :, i-1, j-1]), dtw[:, :, i-1, j])\n",
    "\n",
    "            dtw[:, :, i, j] += value\n",
    "\n",
    "@torch.jit.script\n",
    "def dtw_fast_no_n(x: torch.Tensor, y: torch.Tensor, w: float, eps: float = 1e-5, compute_gradients: bool=True, batched: int = 8):\n",
    "    # shape of x (n, dim, x_len) y (m, dim, y_len)\n",
    "\n",
    "    # performs convolution-like operation, for each kernel the DF\n",
    "    # (of shape (kernel_size, T)) is computed, then summed across channels\n",
    "    # x has shape (batch, c, time_dimension)\n",
    "\n",
    "    # compute pairwise diffs (squared)\n",
    "    p_diff = x[:,None,:,None,:] - y[None,:,:,:,None] # shape (n, n_kernel, d, Kernel_size, T)\n",
    "    euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "\n",
    "    # compute dtw\n",
    "    euc_d[:,:,0,:] = torch.cumsum(euc_d[:,:,0,:], dim=2)\n",
    "    euc_d[:,:,:,0] = torch.cumsum(euc_d[:,:,:,0], dim=2)\n",
    "\n",
    "    if compute_gradients:\n",
    "        # p_diff now contains the partial derivatives of DTW[n, k, i, j] wrt K[k, d, i] (dims (n, k, d, i, j))\n",
    "        p_diff = p_diff / torch.sqrt(euc_d[:,:, None, :, :] + eps)\n",
    "        \n",
    "        grads = torch.zeros((x.shape[0], y.shape[0], y.shape[1], y.shape[2]), device=y.device) # dims (n, k, d, i)\n",
    "        # grads_buffer = torch.empty((num_workers, y.shape[0], y.shape[1], y.shape[2], y.shape[2], x.shape[2]), device=y.device)\n",
    "\n",
    "        for i in range(0, x.shape[0], batched):\n",
    "            initial = i\n",
    "            last = min(initial + batched, x.shape[0])\n",
    "            dtw_compute_all_script(euc_d[initial:last], p_diff[initial:last], grads[initial:last], w)\n",
    "\n",
    "            # processes = [Process(target=dtw_compute_all, args=(euc_d[]))]\n",
    "            # j = min(i+batched, x.shape[0])\n",
    "\n",
    "            # dtw_compute_all(euc_d[i:j], p_diff[i:j], grads[i:j], w)\n",
    "        \n",
    "        return euc_d.sqrt(), grads\n",
    "    else:\n",
    "        dtw_compute_no_grad(euc_d, w)\n",
    "\n",
    "        return euc_d.sqrt(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "[2, 2] [2, 2, 10, 4]\n",
      "tensor(-17.1377) tensor(12.9263)\n"
     ]
    }
   ],
   "source": [
    "a, b = dtw_fast_no_n(x, y, w=0.01, eps=1e-6, batched=32)\n",
    "print(b.min(), b.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssstsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
