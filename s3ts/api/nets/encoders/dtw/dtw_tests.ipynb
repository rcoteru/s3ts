{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def dtw_compute2(dtw: torch.Tensor, dist_grad: torch.Tensor, grad: torch.Tensor, w: float) -> None:\n",
    "    '''\n",
    "        dtw of shape (n, pattern_len, window_size)\n",
    "        dist_grad of shape (n, dims, pattern_len, window_size)\n",
    "        grad of shape (dims, pattern_len)\n",
    "    '''\n",
    "    grads = torch.zeros(dtw.shape[0], dist_grad.shape[1], dtw.shape[1], dtw.shape[1], dtw.shape[2]) # shape (n, dims, pattern_len, pattern_len, window_size)\n",
    "\n",
    "    for i in range(dtw.shape[1]):\n",
    "        grads[:, :, i, i, :] = torch.cumsum(dist_grad[:, :, i, :], dim=1)\n",
    "        grads[:, :, i, i:, 0] = grads[:, :, i, i, :1]\n",
    "\n",
    "    for i in range(1, dtw.shape[1]): # pl\n",
    "        for j in range(1, dtw.shape[2]): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[:, i, j-1], dtw[:, i-1, j-1]), dtw[:, i-1, j])\n",
    "            temp_1 = dtw[:, i, j-1] < dtw[:, i-1, j-1] # path (i, j-1) or (i-1, j)\n",
    "            temp_2 = w * dtw[:, i, j-1] < dtw[:, i-1, j] # path (i, j-1) or (i-1, j-1)\n",
    "            temp_3 = w * dtw[:, i-1, j-1] < dtw[:, i-1, j] # path (i-1, j-1) or (i-1, j)\n",
    "\n",
    "            dtw[:, i, j] += value\n",
    "\n",
    "            grads[temp_1 & temp_2][:, :, i, j] += w * grads[temp_1 & temp_2][:, :, i, j-1]\n",
    "            grads[temp_1 & temp_3][:, :, i, j] += grads[temp_1 & temp_3][:, :, i-1, j]\n",
    "            grads[temp_2 & temp_3][:, :, i, j] += w * grads[temp_2 & temp_3][:, :, i-1, j-1]\n",
    "\n",
    "    grad[:,:] = grads.sum(dim=(0, -2, -1))\n",
    "\n",
    "@torch.jit.script\n",
    "def dtw_compute_one_by_one(dtw: torch.Tensor, dist_grad: torch.Tensor, grad: torch.Tensor, w: float) -> None:\n",
    "    '''\n",
    "        dtw of shape (pattern_len, window_size)\n",
    "        dist_grad of shape (dims, pattern_len, window_size)\n",
    "        grad of shape (dims, pattern_len)\n",
    "    '''\n",
    "    grads = torch.zeros(dist_grad.shape[0], dtw.shape[0], dtw.shape[0], dtw.shape[1]) # shape (dims, pattern_len, pattern_len, window_size)\n",
    "\n",
    "    for i in range(dtw.shape[0]):\n",
    "        grads[:, i, i, :] = torch.cumsum(dist_grad[:, i, :], dim=1)\n",
    "        grads[:, i, i:, 0] = grads[:, i, i, :1]\n",
    "\n",
    "    for i in range(1, dtw.shape[0]): # pl\n",
    "        for j in range(1, dtw.shape[1]): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[i, j-1], dtw[i-1, j-1]), dtw[i-1, j])\n",
    "            temp_1 = dtw[i, j-1] < dtw[i-1, j-1] # path (i, j-1) or (i-1, j)\n",
    "            temp_2 = w * dtw[i, j-1] < dtw[i-1, j] # path (i, j-1) or (i-1, j-1)\n",
    "            temp_3 = w * dtw[i-1, j-1] < dtw[i-1, j] # path (i-1, j-1) or (i-1, j)\n",
    "\n",
    "            dtw[i, j] += value\n",
    "\n",
    "            if temp_1 & temp_2:\n",
    "                grads[:, :, i, j] += w * grads[:, :, i, j-1]\n",
    "            elif temp_1 & temp_3:\n",
    "                grads[:, :, i, j] += grads[:, :, i-1, j]\n",
    "            else:\n",
    "                grads[:, :, i, j] += w * grads[:, :, i-1, j-1]\n",
    "\n",
    "    grad[:,:] += grads.sum(dim=(-2, -1))\n",
    "\n",
    "@torch.jit.script\n",
    "def dtw_compute_everything(dtw: torch.Tensor, dist_grad: torch.Tensor, grad: torch.Tensor, w: float) -> None:\n",
    "    '''\n",
    "        dtw of shape (n, k, pattern_len, window_size)\n",
    "        dist_grad of shape (n, k, dims, pattern_len, window_size)\n",
    "        grad of shape (n, k, dims, pattern_len)\n",
    "    '''\n",
    "    n, k, len_pattern, len_window = dtw.shape\n",
    "    # very big tensor\n",
    "    grads = torch.zeros(n, k, grad.shape[2], len_pattern, len_pattern, len_window) # shape (n, k, dims, pattern_len, pattern_len, window_size)\n",
    "\n",
    "    for i in range(len_pattern):\n",
    "        grads[:, :, :, i, i, :] = torch.cumsum(dist_grad[:, :, :, i, :], dim=-1)\n",
    "        grads[:, :, :, i, i:, 0] = grads[:, :, :, i, i, :1]\n",
    "\n",
    "    for i in range(1, len_pattern): # pl\n",
    "        for j in range(1, len_window): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[:, :, i, j-1], dtw[:, :, i-1, j-1]), dtw[:, :, i-1, j])\n",
    "            temp_1 = dtw[:, :, i, j-1] < dtw[:, :, i-1, j-1] # path (i, j-1) or (i-1, j)\n",
    "            temp_2 = w * dtw[:, :, i, j-1] < dtw[:, :, i-1, j] # path (i, j-1) or (i-1, j-1)\n",
    "            temp_3 = w * dtw[:, :, i-1, j-1] < dtw[:, :, i-1, j] # path (i-1, j-1) or (i-1, j)\n",
    "\n",
    "            dtw[:, :, i, j] += value\n",
    "\n",
    "            grads[temp_1 & temp_2][:, :, :i, i, j] += w * grads[temp_1 & temp_2][:, :, :i, i, j-1]\n",
    "            grads[temp_1 & temp_3][:, :, :i, i, j] += grads[temp_1 & temp_3][:, :, :i, i-1, j]\n",
    "            grads[temp_2 & temp_3][:, :, :i, i, j] += w * grads[temp_2 & temp_3][:, :, :i, i-1, j-1]\n",
    "\n",
    "    grad[:,:,:,:] = grads.sum(dim=(-2, -1))\n",
    "\n",
    "@torch.jit.script\n",
    "def torch_dtw_fast_compute_everything(x: torch.Tensor, y: torch.Tensor, w: float, eps: float = 1e-5):\n",
    "    # shape of x (n, dim, x_len) y (m, dim, y_len)\n",
    "\n",
    "    # performs convolution-like operation, for each kernel the DF\n",
    "    # (of shape (kernel_size, T)) is computed, then summed across channels\n",
    "    # x has shape (batch, c, time_dimension)\n",
    "\n",
    "    # compute pairwise diffs (squared)\n",
    "    p_diff = x[:,None,:,None,:] - y[None,:,:,:,None] # shape (n, n_kernel, d, Kernel_size, T)\n",
    "    euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "\n",
    "    # p_diff now contains the partial derivatives of DTW[n, k, i, j] wrt K[k, d, i] (dims (n, k, d, i, j))\n",
    "    p_diff = p_diff / torch.sqrt(euc_d[:,:, None, :, :] + eps)\n",
    "\n",
    "    # compute dtw\n",
    "    euc_d[:,:,0,:] = torch.cumsum(euc_d[:,:,0,:], dim=2)\n",
    "    euc_d[:,:,:,0] = torch.cumsum(euc_d[:,:,:,0], dim=2)\n",
    "\n",
    "    grads = torch.empty((x.shape[0], y.shape[0], y.shape[1], y.shape[2]))\n",
    "\n",
    "    dtw_compute_everything(euc_d, p_diff, grads, w)\n",
    "\n",
    "    return euc_d.sqrt(), grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_dtw_fast_multiprocess(x: torch.Tensor, y: torch.Tensor, w: float, eps: float = 1e-5, num_processes: int=8):\n",
    "    # shape of x (n, dim, x_len) y (m, dim, y_len)\n",
    "\n",
    "    # performs convolution-like operation, for each kernel the DF\n",
    "    # (of shape (kernel_size, T)) is computed, then summed across channels\n",
    "    # x has shape (batch, c, time_dimension)\n",
    "\n",
    "    # compute pairwise diffs (squared)\n",
    "    p_diff = x[:,None,:,None,:] - y[None,:,:,:,None] # shape (n, n_kernel, d, Kernel_size, T)\n",
    "    euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "\n",
    "    # p_diff now contains the partial derivatives of DTW[n, k, i, j] wrt K[k, d, i] (dims (n, k, d, i, j))\n",
    "    p_diff = p_diff / torch.sqrt(euc_d[:,:, None, :, :] + eps)\n",
    "\n",
    "    # compute dtw\n",
    "    euc_d[:,:,0,:] = torch.cumsum(euc_d[:,:,0,:], dim=2)\n",
    "    euc_d[:,:,:,0] = torch.cumsum(euc_d[:,:,:,0], dim=2)\n",
    "\n",
    "    grads = torch.zeros((num_processes, y.shape[0], y.shape[1], y.shape[2]))\n",
    "\n",
    "    euc_d.share_memory_()\n",
    "    grads.share_memory_()\n",
    "    p_diff.share_memory_()\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    while True:\n",
    "        processes = []\n",
    "        for rank in range(num_processes):\n",
    "            print(\"create\")\n",
    "            p = mp.Process(target=dtw_compute_one_by_one, args=(euc_d[i, j], p_diff[i, j], grads[rank, j], w, ))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            i+=1\n",
    "            if i>x.shape[0]:\n",
    "                i=0\n",
    "                j+=1\n",
    "            if j>y.shape[0]:\n",
    "                break\n",
    "\n",
    "        for p in processes:\n",
    "            p.join()        \n",
    "\n",
    "        if j>y.shape[0]:\n",
    "            break\n",
    "\n",
    "    return euc_d.sqrt(), grads.sum(0)/(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(45)\n",
    "x = torch.randn(100, 9, 50)\n",
    "y = torch.randn(64, 9, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1004.2710) tensor(1017.8133)\n"
     ]
    }
   ],
   "source": [
    "a, b = torch_dtw_fast_compute_everything(x, y, w=0.01, eps=1e-6)\n",
    "print(b.min(), b.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = torch_dtw_fast_multiprocess(x, y, w=0.01, eps=1e-6, num_processes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def dtw_compute_no_n(dtw: torch.Tensor, dist_grad: torch.Tensor, grad: torch.Tensor, w: float) -> None:\n",
    "    '''\n",
    "        dtw of shape (k, pattern_len, window_size)\n",
    "        dist_grad of shape (k, dims, pattern_len, window_size)\n",
    "        grad of shape (k, dims, pattern_len)\n",
    "    '''\n",
    "    k, len_pattern, len_window = dtw.shape\n",
    "    # very big tensor\n",
    "    grads = torch.zeros(k, grad.shape[1], len_pattern, len_pattern, len_window) # shape (n, k, dims, pattern_len, pattern_len, window_size)\n",
    "\n",
    "    for i in range(len_pattern):\n",
    "        grads[:, :, i, i, :] = torch.cumsum(dist_grad[:, :, i, :], dim=2)\n",
    "        grads[:, :, i, i:, 0] = grads[:, :, i, i, :1]\n",
    "\n",
    "    for i in range(1, len_pattern): # pl\n",
    "        for j in range(1, len_window): # ws\n",
    "            value = torch.minimum(w * torch.minimum(dtw[:, i, j-1], dtw[:, i-1, j-1]), dtw[:, i-1, j])\n",
    "            temp_1 = dtw[:, i, j-1] < dtw[:, i-1, j-1] # path (i, j-1) or (i-1, j)\n",
    "            temp_2 = w * dtw[:, i, j-1] < dtw[:, i-1, j] # path (i, j-1) or (i-1, j-1)\n",
    "            temp_3 = w * dtw[:, i-1, j-1] < dtw[:, i-1, j] # path (i-1, j-1) or (i-1, j)\n",
    "\n",
    "            dtw[:, i, j] += value\n",
    "\n",
    "            grads[temp_1 & temp_2][:, :i, i, j] += w * grads[temp_1 & temp_2][:, :i, i, j-1]\n",
    "            grads[temp_1 & temp_3][:, :i, i, j] += grads[temp_1 & temp_3][:, :i, i-1, j]\n",
    "            grads[temp_2 & temp_3][:, :i, i, j] += w * grads[temp_2 & temp_3][:, :i, i-1, j-1]\n",
    "\n",
    "    grad[:,:,:] += grads.sum(dim=(-2, -1))\n",
    "\n",
    "@torch.jit.script\n",
    "def dtw_fast_no_n(x: torch.Tensor, y: torch.Tensor, w: float, eps: float = 1e-5):\n",
    "    # shape of x (n, dim, x_len) y (m, dim, y_len)\n",
    "\n",
    "    # performs convolution-like operation, for each kernel the DF\n",
    "    # (of shape (kernel_size, T)) is computed, then summed across channels\n",
    "    # x has shape (batch, c, time_dimension)\n",
    "\n",
    "    # compute pairwise diffs (squared)\n",
    "    p_diff = x[:,None,:,None,:] - y[None,:,:,:,None] # shape (n, n_kernel, d, Kernel_size, T)\n",
    "    euc_d = torch.square(p_diff).sum(2) # shape (n, n_kernel, kernel_size, T)\n",
    "\n",
    "    # p_diff now contains the partial derivatives of DTW[n, k, i, j] wrt K[k, d, i] (dims (n, k, d, i, j))\n",
    "    p_diff = p_diff / torch.sqrt(euc_d[:,:, None, :, :] + eps)\n",
    "\n",
    "    # compute dtw\n",
    "    euc_d[:,:,0,:] = torch.cumsum(euc_d[:,:,0,:], dim=2)\n",
    "    euc_d[:,:,:,0] = torch.cumsum(euc_d[:,:,:,0], dim=2)\n",
    "\n",
    "    grads = torch.zeros((x.shape[0], y.shape[0], y.shape[1], y.shape[2]))\n",
    "\n",
    "    futures = [torch.jit.fork(dtw_compute_no_n, euc_d[i], p_diff[i], grads[i], w) for i in range(x.shape[0])] \n",
    "    results = [torch.jit.wait(future) for future in futures]\n",
    "\n",
    "    return euc_d.sqrt(), grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1004.2710) tensor(1017.8133)\n"
     ]
    }
   ],
   "source": [
    "a, b = dtw_fast_no_n(x, y, w=0.01, eps=1e-6)\n",
    "print(b.min(), b.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssstsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
