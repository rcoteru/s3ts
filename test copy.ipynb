{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from s3ts.helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset WISDM with a total of 1095616 observations for window size 24\n",
      "Computing medoids...\n",
      "Computing dissimilarity frames...\n",
      "Loading cached dissimilarity frames if available...\n"
     ]
    }
   ],
   "source": [
    "dm = load_dmdataset(\"WISDM\", \"./datasets/\", 0.1, 128, 8, 16, 3, True, 24, 300, [30, 31, 32, 33, 34, 35], True, 1, 1, True, -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3ts.api.nets.wrapper import WrapperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"./training/img_WISDM_30-31-32-33-34-35_16_3_bs128_lr0.001_cnn_gap20_mlp32_1_p24_r0.1_med1_300/version_0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 6, 24, 16])\n",
      "Latent shape:  torch.Size([1, 80])\n",
      "[tensor(80), 32, 6]\n"
     ]
    }
   ],
   "source": [
    "model = WrapperModel.load_from_checkpoint(\n",
    "    hparams_file=f\"./training/img_WISDM_30-31-32-33-34-35_16_3_bs128_lr0.001_cnn_gap20_mlp32_1_p24_r0.1_med1_300/version_3/hparams.yaml\",\n",
    "    checkpoint_path=f\"{modelpath}checkpoints/epoch=0-step=2976-val_re=0.57.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epoch=0-step=2976-val_re=0.57.ckpt', 'epoch=1-step=5952-val_re=0.58.ckpt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(modelpath, \"checkpoints\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./training/img_WISDM_30-31-32-33-34-35_16_3_bs128_lr0.001_cnn_gap20_mlp32_1_p24_r0.1_med1_300/version_3/hparams.yaml\", \"r\") as f:\n",
    "    try:\n",
    "        l = yaml.safe_load(f)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'WISDM',\n",
       " 'dataset_dir': './datasets',\n",
       " 'batch_size': 128,\n",
       " 'num_workers': 8,\n",
       " 'window_size': 16,\n",
       " 'window_stride': 3,\n",
       " 'normalize': True,\n",
       " 'pattern_size': 24,\n",
       " 'compute_n': 300,\n",
       " 'subjects_for_test': [30, 31, 32, 33, 34, 35],\n",
       " 'encoder_architecture': 'cnn_gap',\n",
       " 'decoder_architecture': 'mlp',\n",
       " 'max_epochs': 30,\n",
       " 'lr': 0.001,\n",
       " 'decoder_features': 32,\n",
       " 'encoder_features': 20,\n",
       " 'decoder_layers': 1,\n",
       " 'mode': 'img',\n",
       " 'reduce_imbalance': True,\n",
       " 'label_mode': 1,\n",
       " 'num_medoids': 1,\n",
       " 'voting': 1,\n",
       " 'rho': 0.1,\n",
       " 'use_medoids': True,\n",
       " 'overlap': -1,\n",
       " 'mtf_bins': 10,\n",
       " 'training_dir': 'training',\n",
       " 'n_val_subjects': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(l[\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.utilities.model_summary import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name       | Type                      | Params\n",
       "---------------------------------------------------------\n",
       "0 | encoder    | CNN_GAP_IMG               | 20.1 K\n",
       "1 | decoder    | MultiLayerPerceptron      | 2.8 K \n",
       "2 | flatten    | Flatten                   | 0     \n",
       "3 | softmax    | Softmax                   | 0     \n",
       "4 | train_cm   | MulticlassConfusionMatrix | 0     \n",
       "5 | val_cm     | MulticlassConfusionMatrix | 0     \n",
       "6 | val_auroc  | MulticlassAUROC           | 0     \n",
       "7 | test_cm    | MulticlassConfusionMatrix | 0     \n",
       "8 | test_auroc | MulticlassAUROC           | 0     \n",
       "---------------------------------------------------------\n",
       "22.9 K    Trainable params\n",
       "0         Non-trainable params\n",
       "22.9 K    Total params\n",
       "0.092     Total estimated model params size (MB)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(model, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "tr = Trainer(callbacks=[ModelSummary(max_depth=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3ts.helper_functions import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = get_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = p.parse_args(\"--mode img --dataset WISDM --lr 0.001 --subjects_for_test 30 31 32 33 34 35 --window_size 16 --window_stride 3 --batch_size 128 --encoder_architecture cnn_gap --encoder_features 20 --decoder_architecture mlp --decoder_features 32 --decoder_layers 1 --pattern_size 24 --rho 0.1 --compute_n 300 --use_medoids --num_workers 8 --max_epochs 30 --normalize --reduce_imbalance --n_val_subjects 4\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg2 = eval(str(arg.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'WISDM',\n",
       " 'dataset_dir': './datasets',\n",
       " 'batch_size': 128,\n",
       " 'num_workers': 8,\n",
       " 'window_size': 16,\n",
       " 'window_stride': 3,\n",
       " 'normalize': True,\n",
       " 'pattern_size': 24,\n",
       " 'compute_n': 300,\n",
       " 'subjects_for_test': [30, 31, 32, 33, 34, 35],\n",
       " 'encoder_architecture': 'cnn_gap',\n",
       " 'decoder_architecture': 'mlp',\n",
       " 'max_epochs': 30,\n",
       " 'lr': 0.001,\n",
       " 'decoder_features': 32,\n",
       " 'encoder_features': 20,\n",
       " 'decoder_layers': 1,\n",
       " 'mode': 'img',\n",
       " 'reduce_imbalance': True,\n",
       " 'label_mode': 1,\n",
       " 'num_medoids': 1,\n",
       " 'voting': 1,\n",
       " 'rho': 0.1,\n",
       " 'use_medoids': True,\n",
       " 'overlap': -1,\n",
       " 'mtf_bins': 10,\n",
       " 'training_dir': 'training',\n",
       " 'n_val_subjects': 4}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/mbikandi/Documents/s3ts/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1205/1205 [00:19<00:00, 61.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_auroc         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9053499698638916     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5963507890701294     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2198607921600342     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_pr          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7657859921455383     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_re          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.606015145778656     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_auroc        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9053499698638916    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5963507890701294    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2198607921600342    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_pr         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7657859921455383    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_re         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.606015145778656    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.2198607921600342,\n",
       "  'test_auroc': 0.9053499698638916,\n",
       "  'test_pr': 0.7657859921455383,\n",
       "  'test_re': 0.606015145778656,\n",
       "  'test_f1': 0.5963507890701294}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.test(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 12, 48, 48])\n",
      "Latent shape:  torch.Size([1, 40, 5])\n",
      "[tensor(200), 64, 6]\n"
     ]
    }
   ],
   "source": [
    "from s3ts.api.nets.methods import create_model_from_DM, train_model\n",
    "\n",
    "model = create_model_from_DM(dm, \"img\", \"cnn_gap\", \"mlp\", \"cls\", \"test\", 20, 64, 1, 0.001, voting={\"n\": 1, \"rho\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type                      | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder    | CNN_GAP_IMG               | 18.3 K\n",
      "1 | decoder    | MultiLayerPerceptron      | 13.3 K\n",
      "2 | flatten    | Flatten                   | 0     \n",
      "3 | softmax    | Softmax                   | 0     \n",
      "4 | train_cm   | MulticlassConfusionMatrix | 0     \n",
      "5 | val_cm     | MulticlassConfusionMatrix | 0     \n",
      "6 | val_auroc  | MulticlassAUROC           | 0     \n",
      "7 | test_cm    | MulticlassConfusionMatrix | 0     \n",
      "8 | test_auroc | MulticlassAUROC           | 0     \n",
      "---------------------------------------------------------\n",
      "31.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.5 K    Total params\n",
      "0.126     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3466/3466 [07:59<00:00,  7.23it/s, v_num=18, train_loss_step=0.023, val_loss=0.668, val_auroc=0.935, val_re=0.782, train_loss_epoch=0.0813, train_re=0.969] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3466/3466 [07:59<00:00,  7.23it/s, v_num=18, train_loss_step=0.023, val_loss=0.668, val_auroc=0.935, val_re=0.782, train_loss_epoch=0.0813, train_re=0.969]\n",
      "Input shape:  torch.Size([1, 12, 48, 48])\n",
      "Latent shape:  torch.Size([1, 40, 5])\n",
      "[tensor(200), 64, 6]\n",
      "Validation DataLoader 0: 100%|██████████| 1207/1207 [01:00<00:00, 19.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_auroc         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9592697620391846     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_f1           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8081207871437073     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5647413730621338     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_pr           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.850165843963623     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_re           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8028659820556641     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_auroc        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9592697620391846    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_f1          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8081207871437073    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5647413730621338    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_pr          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.850165843963623    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_re          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8028659820556641    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(WrapperModel(\n",
       "   (encoder): CNN_GAP_IMG(\n",
       "     (cnn_0): Sequential(\n",
       "       (0): Conv2d(12, 10, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "       (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU()\n",
       "       (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "     (cnn_1): Sequential(\n",
       "       (0): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "       (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU()\n",
       "       (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "     (cnn_2): Sequential(\n",
       "       (0): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "       (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU()\n",
       "       (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     )\n",
       "   )\n",
       "   (decoder): MultiLayerPerceptron(\n",
       "     (fcn_layer_0): Linear(in_features=200, out_features=64, bias=True)\n",
       "     (act_layer_0): ReLU()\n",
       "     (fcn_out): Linear(in_features=64, out_features=6, bias=True)\n",
       "   )\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (softmax): Softmax(dim=None)\n",
       "   (train_cm): MulticlassConfusionMatrix()\n",
       "   (val_cm): MulticlassConfusionMatrix()\n",
       "   (val_auroc): MulticlassAUROC()\n",
       "   (test_cm): MulticlassConfusionMatrix()\n",
       "   (test_auroc): MulticlassAUROC()\n",
       " ),\n",
       " {'val_re': 0.8028659820556641,\n",
       "  'val_f1': 0.8081207871437073,\n",
       "  'val_auroc': 0.9592697620391846})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(dm, model, 3, {\n",
    "    \"default_root_dir\": \"training\",\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"seed\": 42\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_online = tm.Accuracy(task=\"multiclass\", num_classes=6, average=\"macro\")\n",
    "cm_online = tm.ConfusionMatrix(task=\"multiclass\", num_classes=6)\n",
    "dataloader = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch_idx, data in enumerate(dataloader):\n",
    "    preds = model.logits(data[\"series\"])\n",
    "    acc_online(preds, data[\"label\"])\n",
    "    cl = torch.argmax(preds, dim=1)\n",
    "    cm_online(cl, data[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8273)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_online.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm_online.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm.diag()\n",
    "FP = cm.sum(0) - TP\n",
    "FN = cm.sum(1) - TP\n",
    "TN = torch.empty(cm.shape[0])\n",
    "for i in range(cm.shape[0]):\n",
    "    TN[i] = cm[:i,:i].sum() + cm[:i,i:].sum() + cm[i:,:i].sum() + cm[i:,i:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9405, 0.9863, 0.9907, 0.9917, 0.9052, 0.9293])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6781, 0.9370, 0.8843, 0.8330, 0.8169, 0.8146])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8273)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssstsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
